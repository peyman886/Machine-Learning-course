{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size='+1' color=red>**Attention:**</font> Data cleaning and other parts of preprocessing of data which we covered in the first assignment, is not neccesary all the time but you may need some of them according to task at hand. So we don't explicitly mention them each time. This is your job to figure out when to apply them."
      ],
      "metadata": {
        "id": "xvYT4kv1TQpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q1:**</font> <font size='+2'> **Multivariate least squares** </font>\n",
        "\n",
        "In a regression task, suppose that instead of trying to predict a single output, we have a training set with multiple outputs for each example:\n",
        "\n",
        "$$\\{(x^{(i)}, y^{(i)}), i=1, ...,m\\}, x^{(i)} ‚àà \\mathbb{R}^n, y^{(i)} ‚àà \\mathbb{R}^p. $$\n",
        "\n",
        "Thus for each training example, $y^{(i)}$ is vector-valued, with $p$ entries. We wish to use a linear model to predict the outputs, as in least squares, by specifying the parameter matrix $ùöØ$ in\n",
        "\n",
        "$$y = ùöØ^T x,$$\n",
        "\n",
        "where $ùöØ ‚àà \\mathbb{R}^{n√óp}$.\n",
        "\n",
        "<font size='+1'>**(a)**</font> The cost function for this case is\n",
        "\n",
        "$$J(ùöØ) = \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^p \\Big((ùöØ^Tx^{(i)})_j - y^{(i)}_j\\Big)^2.$$\n",
        "\n",
        "Write $J(ùöØ)$ in matrix-vector notation (i.e., without using any summations). [Hint: Start with the $m √ó n$ design matrix\n",
        "\n",
        "$$\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "\\overline{\\space\\space\\space\\space\\space\\space} & (x^{(1)})^T & \\overline{\\space\\space\\space\\space\\space\\space}\\\\\n",
        "\\overline{\\space\\space\\space\\space\\space\\space} & (x^{(2)})^T & \\overline{\\space\\space\\space\\space\\space\\space}\\\\\n",
        "&\\vdots&\\\\\n",
        "\\overline{\\space\\space\\space\\space\\space\\space} & (x^{(m)})^T & \\overline{\\space\\space\\space\\space\\space\\space}\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "and the $m √ó p$ target matrix\n",
        "\n",
        "$$\n",
        "Y =\n",
        "\\begin{bmatrix}\n",
        "\\overline{\\space\\space\\space\\space\\space\\space} & (y^{(1)})^T & \\overline{\\space\\space\\space\\space\\space\\space}\\\\\n",
        "\\overline{\\space\\space\\space\\space\\space\\space} & (y^{(2)})^T & \\overline{\\space\\space\\space\\space\\space\\space}\\\\\n",
        "&\\vdots&\\\\\n",
        "\\overline{\\space\\space\\space\\space\\space\\space} & (y^{(m)})^T & \\overline{\\space\\space\\space\\space\\space\\space}\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "and then work out how to express $J(ùöØ)$ in terms of these matrices.]\n",
        "\n",
        "<font size='+1'>**(b)**</font> Find the closed form solution for $ùöØ$ which minimizes $J(ùöØ)$. This is the equivalent to the normal equations for the multivariate case.\n",
        "\n",
        "<font size='+1'>**(c)**</font> Suppose instead of considering the multivariate vectors $y^{(i)}$ all at once, we instead compute each variable $y^{(i)}_j$ separately for each $j = 1,\\dots,p$. In this case, we have a $p$ individual linear models, of the form\n",
        "\n",
        "$$ y^{(i)}_j = Œ∏^T_j x^{(i)}, j = 1, \\dots , p. $$\n",
        "\n",
        "(So here, each $Œ∏_j ‚àà \\mathbb{R}^n$). How do the parameters from these $p$ independent least squares problems compare to the multivariate solution?"
      ],
      "metadata": {
        "id": "9Oz3V9YijF3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A1:**</font> Part (a), (b), and (c)"
      ],
      "metadata": {
        "id": "Gk3zryb_jqYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the normal equations of multivariate case is simillar to the univariate case. Using this normal equation we can find the model parameters directly without the need of gradient descent.\n",
        "\n",
        "<font size='+1'>**(d)**</font> At first, try to implement linear regression model using the normal equation obtained from the previous question. Secondly, consider [this](https://www.kaggle.com/datasets/kyasar/boston-housing) dataset. Split the data to 70-15-15 and try to fit your model on training set in order to predict the `crim` and `tax` columns. Then evaluate it using the test set. Report the loss and accuracy of each split of the data."
      ],
      "metadata": {
        "id": "W8aO3NvkPw5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A1:**</font> Part (d) <br> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "NKoYFrhIX2rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q2:**</font> <font size='+2'> **$l^1$ and $l^2$-Regularization** </font>\n",
        "\n",
        "When training a machine learning model, it's possible for the model to exhibit high accuracy on the training set while performing poorly on the test data.\n",
        "\n",
        "This situation indicates a potential issue with *overfitting*. Overfitting occurs when the machine learning model attempts to fit the training data too closely, capturing not only the underlying patterns but also noise and inaccuracies present in the dataset. Consequently, the model becomes less effective and accurate when applied to new, unseen data.\n",
        "\n",
        "To address overfitting, various methods can be employed. In the context of Linear Regression, one approach involves the utilization of regularization techniques, commonly known as $l^1$ and $l^2$-regularization methods.\n",
        "\n",
        "<font size='+1'>**(a)**</font> First, write down the cost function for $l^1$ and $l^2$-regularization and with the help of the following image, try to explain their differences.\n",
        "\n",
        "<font size='+1'>**(b)**</font> Imagine feature selection is crucial for your task, which regularization technique will you use? Explain the reason behind of your decision and try to justify your answer using the derivative of the cost functions which you wrote for the part (a).\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:800/1*ZeINTX82W7vwqLMHHWEaTQ.jpeg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PvVpPcQSYpJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A2:**</font> Parts (a) and (b)"
      ],
      "metadata": {
        "id": "U5u3UBXpu856"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size='+1'>**(c)**</font> In this part we are going to see the impact of each regularization technique and to get more familiar with concept of overfitting and underfitting.\n",
        "First, generate the relevant data using the following codes.\n"
      ],
      "metadata": {
        "id": "0gb_GxDm_k5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.arange(-10,10,0.2)\n",
        "Y = 2*np.cos(x)/-np.pi + (2*x)/(2*np.pi)+2*np.cos(3*x)/(-3*np.pi)"
      ],
      "metadata": {
        "id": "ltN3Jgjsr6ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, add white Gaussian noise to the data with an impact factor of $0.1$. Try to fit a function of degree 1 to 15 to the data."
      ],
      "metadata": {
        "id": "BwMmx7EqzUrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = 0\n",
        "std = 1\n",
        "# Your code:"
      ],
      "metadata": {
        "id": "-2AbrSrC9vcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Determine the best and worst degree in terms of the total cost."
      ],
      "metadata": {
        "id": "JP79Y6sLzUrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code:"
      ],
      "metadata": {
        "id": "eirQUDLwzUrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Draw the fit diagram for the best and worst degrees and also the degree of 1, 3, 8, and 15 and report the MSE values."
      ],
      "metadata": {
        "id": "Dy41IaQpzUrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code:"
      ],
      "metadata": {
        "id": "lbqD6WlYzUrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Describe the results of your observations by mentioning the values of bias and variance"
      ],
      "metadata": {
        "id": "wuYH_vzFzUrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code:"
      ],
      "metadata": {
        "id": "Yd_wMY1PzUrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now, we only consider the degree of 15  in order to see the impact of regularization on overfitting. Plot the data, your estimated function (degree of 15), and the true function Y in a single plot."
      ],
      "metadata": {
        "id": "uIdaQQxhwC0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code:"
      ],
      "metadata": {
        "id": "SV6WbSIAv3fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer these questions:\n",
        "1. Explain why does overfitting happen?\n",
        "2. Discuss about the training error and generalization error of this model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLERHD_n1Ieq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A2:**</font>"
      ],
      "metadata": {
        "id": "afwfeYL3xOQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you should do:\n",
        "1. Add $l^2$ regularizer to your model ($Œª=1$).\n",
        "2. Plot data, your estimated function, and the true function in a single plot."
      ],
      "metadata": {
        "id": "VeorJ26n1_0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code:"
      ],
      "metadata": {
        "id": "L7ALHvN3xejW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move on to $l^1$, let's see its impact. You should do the following steps:\n",
        "1. Add $l^1$ regularizer to your model ($Œª=1$).\n",
        "2. Plot data, your estimated function, and the true function in a single plot."
      ],
      "metadata": {
        "id": "3VpvSE_N3aWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each model, plot a histogram of the value of their parameters ($w_i$)."
      ],
      "metadata": {
        "id": "fwuZ71Fk3nGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## plot (you can use subplot)"
      ],
      "metadata": {
        "id": "ucAArLgRqs2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer these questions:\n",
        "\n",
        "1. Compare the results of models.\n",
        "\n",
        "2. Explain the impact of each regularizer?\n",
        "\n",
        "2. Which one is the best estimate of the true function? Why?\n",
        "\n",
        "3. Compare the histograms and discuss their differences.\n",
        "\n",
        "4. Which regularizer do you think is more robust to outlier data? Justify your answer."
      ],
      "metadata": {
        "id": "Hr9seqLT4Ox3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A2:**</font>"
      ],
      "metadata": {
        "id": "-eXeyDpWwKR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is another regularization technique called *elastic net regularization* which uses both $l^1$ and $l^2$ regularization terms. Elastic net linearly combines $l^1$ and $l^2$ penalty terms and tries to prevent the model from overfitting. Try to regularize the model using elastic net ($Œª_1=Œª_2=1$) and then plot data, your estimated function, and the true function in a single plot."
      ],
      "metadata": {
        "id": "q3mSUsMEzySr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "JDYEbReO83vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q3:**</font> <font size='+2'> **Bias-Variance Trade-off** </font>\n",
        "\n",
        "Prove the below equation, which is called bias-variance decomposition:\n",
        "\n",
        "$$\\mathbb{E}[(y‚àíf_s(x))^2] = (y ‚àí \\mathbb{E}[f_s(x)] )^2 + \\mathbb{E} [(\\mathbb{E}[f_s(x)]‚àíf_s(x))^2]$$\n",
        "\n",
        "Using the above equation, interpret the following figure.\n",
        "\n",
        "![](https://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
        "\n",
        "- In which area of the figure underfitting and overfitting will happen? why?\n",
        "\n",
        "- Talk about the training error and generalization error w.r.t above figure and explain its relationship with bias and variance."
      ],
      "metadata": {
        "id": "HDPeKHgfZng6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A3:**</font>"
      ],
      "metadata": {
        "id": "xHNVQdxSxDkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q4:**</font> <font size='+2'> **Circle** </font>\n",
        "\n",
        "In this problem, we will use logistic regression to separate points inside a circle from the outside ones. The code below uses the random functions of the NumPy library to uniformly generate data inside a square A*A centered at (x1_0, x2_0). Points that lie inside the circle (x1_0,x2_0,R) get labeled 1 and vice versa."
      ],
      "metadata": {
        "id": "CEj0x5KyO_SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "iTx5gssIbj9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1_0 = 10\n",
        "x2_0 = 10\n",
        "A = 5\n",
        "N = 20000\n",
        "R = 2\n",
        "\n",
        "x1 = A *np.random.rand(N) + x1_0 - A/2\n",
        "x2 = A *np.random.rand(N) + x2_0 - A/2\n",
        "rr = np.square(x1-x1_0) + np.square(x2-x2_0)\n",
        "label = rr<= R**2\n",
        "\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.scatter(x1[label==1], x2[label==1],c='r', s= 1, label=\"label= 1\")\n",
        "plt.scatter(x1[label==0], x2[label==0],c='b' ,s =1, label=\"label= 0\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3w9an9kRb3-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the logistic regression model in Scikit-learn library we want to classify these points.\n",
        "\n",
        "### Step 1\n",
        "In this step, use x1 and x2 as input features.\n",
        "\n",
        "You should do:\n",
        "\n",
        "1. Split the data into the test set and train set (with ratio = 0.5).\n",
        "\n",
        "2. Create a logistic regression model (without penalty).\n",
        "\n",
        "2. Feed the training points into your model and train it.\n",
        "\n",
        "3. Using your trained model, predict labels of the test set.\n",
        "\n",
        "4. Plot the data such that their color represents their predicted label (like the previous image)."
      ],
      "metadata": {
        "id": "esJEoSRSrCH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model and predict labels"
      ],
      "metadata": {
        "id": "n_qSNXXIduAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot"
      ],
      "metadata": {
        "id": "l_hyfLKMti54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer these questions:\n",
        "\n",
        "1. Explain what logistic regression is.\n",
        "\n",
        "2. Write its formula for this problem (be specific about the dimension of the variables).\n",
        "\n",
        "3. Learning is all about finding unknown parameters of the model to make it fit the training data. Point out the unknown parameters of your logistic regression model.\n"
      ],
      "metadata": {
        "id": "hM7jhQYY5Ir8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can probably see, your model is a big mess.\n",
        "Can you guess why?\n"
      ],
      "metadata": {
        "id": "Dj-uiHfi5xYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer these questions:\n",
        "\n",
        "1. With respect to your model, specify TP, TN, FP, and FN points. (plot them in 4 different colors)\n",
        "\n",
        "2. Report the following metrics for your model: accuracy, precision (sensitivity), recall, and specificity."
      ],
      "metadata": {
        "id": "_JSdr4vv2oK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot TP, TN, FP, FN"
      ],
      "metadata": {
        "id": "HK0i_icNCOTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## calculate metrics"
      ],
      "metadata": {
        "id": "bqxn9w7_7dlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2\n",
        "\n",
        "Now it is time to do some serious work!\n",
        "\n",
        "To train a model that actually works, we need to use something called [Kernel](https://en.wikipedia.org/wiki/Kernel_method).\n",
        "To put it simply, Kernel is a transformation that transforms input space into a feature space. By mapping the inputs to the feature space, we would have some features that are more usable for our model. In other words, sometimes our model is not that complex to extract those features by itself (e.g., linear regression), so Kernel does it for the model.\n",
        "\n",
        "You should do:\n",
        "\n",
        "1. Find a proper kernel for our problem (be careful! the circle was not centered at (0,0)).\n",
        "2. Convert the inputs using the Kernel.\n",
        "3. Feed the resulting features into the model and train it.\n",
        "4. Evaluate your model on the test set (plot points with the predicted labels).\n",
        "5. What is the accuracy of your model now? (Congratulation!!)\n"
      ],
      "metadata": {
        "id": "JyqvgbqeCmZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## kernel"
      ],
      "metadata": {
        "id": "bDiXS7cAfBj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## train and predict"
      ],
      "metadata": {
        "id": "swQjyqLFe23T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## plot"
      ],
      "metadata": {
        "id": "cvAVvX1z7qEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## calculate accuracy"
      ],
      "metadata": {
        "id": "WyGs2ZHpLnmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q5:**</font> <font size='+2'> **Logistic Regression** </font>\n",
        "\n",
        "In this question you need to implement logistic regression from scratch. Then you will use it and try to reproduce the results of previous question."
      ],
      "metadata": {
        "id": "ziUl0LyAONcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "  def __init__(self, lr, num_iter, random_state=42):\n",
        "    \"\"\"\n",
        "    lr: learning rate\n",
        "    num_iter: number of iterations over the training data\n",
        "    random_state: random state to intialize random weights, don't change the 42\n",
        "    \"\"\"\n",
        "    self.lr = lr\n",
        "    self.num_iter = num_iter\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "        Parameters\n",
        "        -------\n",
        "        X : Training data, shape = [n_examples, n_features]\n",
        "        y : Target values, shape = [n_examples]\n",
        "\n",
        "        return\n",
        "        -------\n",
        "        self : Instance of LogisticRegression\n",
        "    \"\"\"\n",
        "\n",
        "    # intilize your weights using the normal distribution (mean=0, std=0.01)\n",
        "    # you should use weights[0] as bias and weights[1:] as non-bias weights\n",
        "    self.weights = # TODO\n",
        "\n",
        "    self.losses = list()\n",
        "\n",
        "    for i in range(self.num_iter):\n",
        "        f_x = # TODO\n",
        "        output = self.activation(f_x)\n",
        "        # calculate the error and update the model parameters\n",
        "        # TODO\n",
        "\n",
        "        # calculate the cost\n",
        "        loss = # TODO\n",
        "\n",
        "        self.losses.append(loss)\n",
        "\n",
        "    return self\n",
        "\n",
        "  def activation(self):\n",
        "    # Compute logistic sigmoid activation\n",
        "    pass\n",
        "\n",
        "  def predict(self):\n",
        "    # Return class label\n",
        "    pass"
      ],
      "metadata": {
        "id": "9vs3Yhj8OvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use your implementation to reproduce the results of previous question. Also, try to plot the learning curve of your training process."
      ],
      "metadata": {
        "id": "P0JaCPLNeDko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "L2xJVTRmemmN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}