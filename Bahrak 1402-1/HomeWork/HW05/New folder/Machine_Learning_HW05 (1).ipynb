{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size='+1' color=red>**Attention:**</font> Data cleaning and other parts of preprocessing of data which we covered in the first assignment, is not neccesary all the time but you may need some of them according to task at hand. So we don't explicitly mention them each time. This is your job to figure out when to apply them.\n",
        "\n",
        "<font size='+1' color=red>**Attention 2:**</font> For your implementations always use `random_state=42` so your code would be reproducible."
      ],
      "metadata": {
        "id": "xvYT4kv1TQpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q1:**</font> <font size='+2'> **PCA for Classification** </font>\n",
        "\n",
        "In this question we want to work with the Fashion-MNIST dataset. Fashion-MNIST is a dataset comprising of $28 \\times 28$ grayscale images of $70,000$ fashion products from $10$ categories, with $7,000$ images per category. The training set has $60,000$ images and the test set has $10,000$ images. <br>\n",
        "<font color=red>**Note:**</font> You can download it from any source you want. <br>\n",
        "<font color=red>**Note:**</font> Take first $60,000$ instances of it as the train and the $10,000$ remaining instances as the test set.\n",
        "\n",
        "Using explained varinace ratio and considering a threshold like $95\\%$ you probably know how to choose the right number of dimensions to perform PCA. But, when you are using dimensionality reduction as a preprocessing step for a supervised learning task, it is important to consider the impact of the optimal number of dimensions on the overall performance of the model. Consider the classification task using the dataset at hand. Try to find the best number of components for the PCA with respect to the task. You should use the `RandomForestClassifier`, `KNeighborsClassifier`, `DecisionTreeClassifier`, and `AdaBoostClassifier`. Compare your results (number of dimensions, accuracy, precision, recall, f1-score, and confusion matrix) and explain why the number of dimensions for different models are different. Don't forget to analyze your results. [Hint: you should try to make a pipeline and try to tune the hyperparameters of PCA and your model adjointly.]\n",
        "\n",
        "At the end, perform the hyperparameter tuning but this time without considering the PCA preprocessing step. Compare your results with previous ones."
      ],
      "metadata": {
        "id": "PvVpPcQSYpJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A1:**</font>"
      ],
      "metadata": {
        "id": "8rOUtFP1DHAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Necessary Libraries\n"
      ],
      "metadata": {
        "id": "yuEirALGZoia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "# from scipy.stats import randint"
      ],
      "metadata": {
        "id": "Ryc62iG4ZfJ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load the Dataset"
      ],
      "metadata": {
        "id": "YeFCAEXPZ0BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "fashion_mnist = fetch_openml('Fashion-MNIST', version=1)\n",
        "X = fashion_mnist.data\n",
        "y = fashion_mnist.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ],
      "metadata": {
        "id": "g1tGNFcUWfVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9863ac1-ee7d-40e0-f92d-5d70ed52dd80"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define the Pipelines and Parameters for Grid Search"
      ],
      "metadata": {
        "id": "1aB3QaZJZ4yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipelines = {\n",
        "    'rf': Pipeline([('pca', PCA()), ('classifier', RandomForestClassifier(random_state=42))]),\n",
        "    'knn': Pipeline([('pca', PCA()), ('classifier', KNeighborsClassifier())]),\n",
        "    'dt': Pipeline([('pca', PCA()), ('classifier', DecisionTreeClassifier(random_state=42))]),\n",
        "    'ada': Pipeline([('pca', PCA()), ('classifier', AdaBoostClassifier(random_state=42))])\n",
        "}\n",
        "\n",
        "param_grid = {\n",
        "    'rf': {'pca__n_components': [0.85, 0.90, 0.95], 'classifier__n_estimators': [100, 200]},\n",
        "    'knn': {'pca__n_components': [0.85, 0.90, 0.95], 'classifier__n_neighbors': [3, 5, 7]},\n",
        "    'dt': {'pca__n_components': [0.85, 0.90, 0.95], 'classifier__max_depth': [10, 20, 30]},\n",
        "    'ada': {'pca__n_components': [0.85, 0.90, 0.95], 'classifier__n_estimators': [50, 100]}\n",
        "}\n"
      ],
      "metadata": {
        "id": "Vaj9v_ZVZlIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train and Evaluate Each Model"
      ],
      "metadata": {
        "id": "8BZ6F525Z-zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for name, pipeline in pipelines.items():\n",
        "    # grid_search = GridSearchCV(pipeline, param_grid[name], cv=5, scoring='accuracy')\n",
        "    grid_search = RandomizedSearchCV(pipeline, param_grid[name], n_iter=5, cv=5, verbose=1, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    predictions = best_model.predict(X_test)\n",
        "\n",
        "    results[name] = {\n",
        "        'Best Parameters': grid_search.best_params_,\n",
        "        'Number of PCA Components': best_model.named_steps['pca'].n_components_,\n",
        "        'Accuracy': accuracy_score(y_test, predictions),\n",
        "        'Precision': precision_score(y_test, predictions, average='macro'),\n",
        "        'Recall': recall_score(y_test, predictions, average='macro'),\n",
        "        'F1 Score': f1_score(y_test, predictions, average='macro'),\n",
        "        'Confusion Matrix': confusion_matrix(y_test, predictions)\n",
        "    }\n",
        "\n",
        "# Display results\n",
        "for model, metrics in results.items():\n",
        "    print(f\"Model: {model}\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "d4iNu_LTaEcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Analysis"
      ],
      "metadata": {
        "id": "z-9osEWuYd-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RandomForestClassifier (rf)\n",
        "- **PCA Components**: 0.9\n",
        "- **Number of PCA Components**: 84\n",
        "- **Accuracy**: 0.8624\n",
        "- **Precision**: 0.8609\n",
        "- **Recall**: 0.8624\n",
        "- **F1 Score**: 0.8608\n",
        "\n",
        "**Analysis**: RandomForest performed well with a PCA component setting of 0.9. This indicates that capturing 90% of the variance in the data is sufficient for a complex ensemble method like RandomForest, which can handle a higher dimensional space effectively. The high accuracy and balanced precision-recall suggest that RandomForest is able to classify most classes accurately.\n",
        "\n",
        "#### KNeighborsClassifier (knn)\n",
        "- **PCA Components**: 0.95\n",
        "- **Accuracy**: 0.8623\n",
        "- **Precision**: 0.8631\n",
        "- **Recall**: 0.8623\n",
        "- **F1 Score**: 0.8616\n",
        "\n",
        "**Analysis**: KNN needed slightly more features (95% variance) to achieve similar performance to RandomForest. This is understandable as KNN relies heavily on the feature space for making decisions based on the nearest neighbors. The slight increase in the number of dimensions may provide more distinctiveness between different classes for KNN.\n",
        "\n",
        "#### DecisionTreeClassifier (dt)\n",
        "- **PCA Components**: 0.85\n",
        "- **Accuracy**: 0.7753\n",
        "- **Precision**: 0.7770\n",
        "- **Recall**: 0.7753\n",
        "- **F1 Score**: 0.7760\n",
        "\n",
        "**Analysis**: The DecisionTreeClassifier performed best with 85% variance, which is lower than the others. This might be because decision trees can overfit with too many features. A reduced feature set can sometimes help in preventing the model from fitting to noise in the data.\n",
        "\n",
        "#### AdaBoostClassifier (ada)\n",
        "- **PCA Components**: 0.95\n",
        "- **Accuracy**: 0.5746\n",
        "- **Precision**: 0.5769\n",
        "- **Recall**: 0.5746\n",
        "- **F1 Score**: 0.5626\n",
        "\n",
        "**Analysis**: AdaBoost with 95% variance components had the lowest performance among the models. AdaBoost is sensitive to noisy data and outliers, and the higher dimensional space might be introducing more complexity than the model can handle effectively.\n",
        "\n",
        "\n",
        "#### **Overall Observation**\n",
        "Different models require a different number of PCA components because each model has a unique way of handling and interpreting features. Models like RandomForest and KNN can benefit from a higher dimensional space as they can capture more complex patterns, while simpler models like DecisionTrees might perform better with fewer dimensions to prevent overfitting.\n",
        "\n",
        "The varying performance across models also highlights the importance of considering both the model's nature and the feature space's dimensionality when performing tasks like classification. The results also demonstrate the utility of PCA in reducing dimensionality while preserving enough information for effective model training and prediction."
      ],
      "metadata": {
        "id": "3CeQ7wRXYUDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. tuning without considering the PCA preprocessing step"
      ],
      "metadata": {
        "id": "AjVeOqHGaB8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1 Code"
      ],
      "metadata": {
        "id": "zJlyGko5qO3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {\n",
        "    'rf': RandomForestClassifier(random_state=42),\n",
        "    'knn': KNeighborsClassifier(),\n",
        "    'dt': DecisionTreeClassifier(random_state=42),\n",
        "    'ada': AdaBoostClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "param_grid_no_pca = {\n",
        "    'rf': {'n_estimators': [100, 200]},\n",
        "    'knn': {'n_neighbors': [3, 5, 7]},\n",
        "    'dt': {'max_depth': [10, 20, 30]},\n",
        "    'ada': {'n_estimators': [50, 100]}\n",
        "}"
      ],
      "metadata": {
        "id": "fMWlc_UlaTXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_no_pca = {}\n",
        "for name, classifier in classifiers.items():\n",
        "    grid_search_no_pca = RandomizedSearchCV(classifier, param_grid_no_pca[name],\n",
        "                                            scoring='accuracy', n_iter=5, cv=5, verbose=1, n_jobs=-1)\n",
        "    grid_search_no_pca.fit(X_train, y_train)\n",
        "\n",
        "    best_model_no_pca = grid_search_no_pca.best_estimator_\n",
        "    predictions_no_pca = best_model_no_pca.predict(X_test)\n",
        "\n",
        "    results_no_pca[name] = {\n",
        "        'Best Parameters': grid_search_no_pca.best_params_,\n",
        "        'Number of PCA Components': best_model.named_steps['pca'].n_components_,\n",
        "        'Accuracy': accuracy_score(y_test, predictions_no_pca),\n",
        "        'Precision': precision_score(y_test, predictions_no_pca, average='macro'),\n",
        "        'Recall': recall_score(y_test, predictions_no_pca, average='macro'),\n",
        "        'F1 Score': f1_score(y_test, predictions_no_pca, average='macro'),\n",
        "        'Confusion Matrix': confusion_matrix(y_test, predictions_no_pca)\n",
        "    }\n",
        "\n",
        "# Display results\n",
        "for model, metrics in results_no_pca.items():\n",
        "    print(f\"Model: {model}\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "a992MUgPaXpO",
        "outputId": "cb2d40c5-414c-4232-83ed-66eceecd012e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 2 is smaller than n_iter=5. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 2 is smaller than n_iter=5. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "Model: rf\n",
            "Best Parameters: {'n_estimators': 200}\n",
            "Accuracy: 0.8775\n",
            "Precision: 0.8765315291845133\n",
            "Recall: 0.8775000000000001\n",
            "F1 Score: 0.8761952314481413\n",
            "Confusion Matrix: [[860   0  11  27   4   1  84   0  13   0]\n",
            " [  3 965   3  21   2   0   5   0   1   0]\n",
            " [ 11   0 800   9 117   0  60   0   3   0]\n",
            " [ 17   3  10 904  34   0  30   0   2   0]\n",
            " [  0   0  91  33 823   0  51   0   2   0]\n",
            " [  0   0   0   1   0 959   0  29   2   9]\n",
            " [151   1 123  29  88   0 592   0  16   0]\n",
            " [  0   0   0   0   0  11   0 953   0  36]\n",
            " [  1   1   4   4   5   2   5   3 974   1]\n",
            " [  0   0   0   0   0   9   1  43   2 945]]\n",
            "\n",
            "\n",
            "Model: knn\n",
            "Best Parameters: {'n_neighbors': 5}\n",
            "Accuracy: 0.8554\n",
            "Precision: 0.8578152450755354\n",
            "Recall: 0.8554\n",
            "F1 Score: 0.8546439722018905\n",
            "Confusion Matrix: [[855   1  17  16   3   1 100   1   6   0]\n",
            " [  8 968   4  12   4   0   3   0   1   0]\n",
            " [ 24   2 819  11  75   0  69   0   0   0]\n",
            " [ 41   8  15 860  39   0  34   0   3   0]\n",
            " [  2   1 126  26 773   0  71   0   1   0]\n",
            " [  1   0   0   0   0 822   5  96   1  75]\n",
            " [176   1 132  23  80   0 575   0  13   0]\n",
            " [  0   0   0   0   0   3   0 961   0  36]\n",
            " [  2   0  10   4   7   0  16   7 953   1]\n",
            " [  0   0   0   0   0   2   1  29   0 968]]\n",
            "\n",
            "\n",
            "Model: dt\n",
            "Best Parameters: {'max_depth': 10}\n",
            "Accuracy: 0.8009\n",
            "Precision: 0.8046192725818088\n",
            "Recall: 0.8009000000000001\n",
            "F1 Score: 0.801520058199\n",
            "Confusion Matrix: [[774   3  25  40  12   0 132   1   9   4]\n",
            " [ 13 924   4  40   7   0  10   0   1   1]\n",
            " [ 13   1 665   8 242   0  62   0   8   1]\n",
            " [ 32  11  26 811  62   1  50   0   6   1]\n",
            " [  4   0 124  39 734   0  96   0   3   0]\n",
            " [  2   4   0   1   1 864   0  70  21  37]\n",
            " [135   4 164  38 140   2 499   0  15   3]\n",
            " [  0   0   0   0   0  27   0 928   6  39]\n",
            " [  4   2  17  10  20   7  27   9 902   2]\n",
            " [  1   0   1   0   0  21   1  65   3 908]]\n",
            "\n",
            "\n",
            "Model: ada\n",
            "Best Parameters: {'n_estimators': 50}\n",
            "Accuracy: 0.5425\n",
            "Precision: 0.5612784019025104\n",
            "Recall: 0.5425\n",
            "F1 Score: 0.5182906120607471\n",
            "Confusion Matrix: [[ 47  21 302 296  26   1 297   0   8   2]\n",
            " [  3 804  37 143   8   1   4   0   0   0]\n",
            " [  9   5 604  35 304   0  34   0   9   0]\n",
            " [  7  90 116 659 115   0  10   0   3   0]\n",
            " [  7   5 342  41 595   0   7   0   3   0]\n",
            " [  0   0   0   0   1 557   0 313  47  82]\n",
            " [ 19  10 483 139 237   0  93   0  19   0]\n",
            " [  0   0   0   0   0  83   0 901   4  12]\n",
            " [ 31   1  12   8   9  20 112  53 751   3]\n",
            " [  0   0   0   1   0 111   1 464   9 414]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2 Analysis"
      ],
      "metadata": {
        "id": "5QrKVIFiqW5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### RandomForestClassifier (rf)\n",
        "- **With PCA**: Accuracy = 0.8624, PCA Components = 0.9, Number of PCA Components = 84\n",
        "- **Without PCA**: Accuracy = 0.8775\n",
        "- **Observation**: RandomForest showed a slight improvement in accuracy without PCA. This suggests that the original feature set contains useful information that gets lost during dimensionality reduction.\n",
        "\n",
        "##### KNeighborsClassifier (knn)\n",
        "- **With PCA**: Accuracy = 0.8623, PCA Components = 0.95, Number of PCA Components = 187\n",
        "- **Without PCA**: Accuracy = 0.8554\n",
        "- **Observation**: KNN performed slightly better with PCA. This indicates that reducing the dimensions helped KNN to process the data more effectively, possibly by removing noise or irrelevant features.\n",
        "\n",
        "##### DecisionTreeClassifier (dt)\n",
        "- **With PCA**: Accuracy = 0.7753, PCA Components = 0.85, PCA Number of PCA Components = 43\n",
        "- **Without PCA**: Accuracy = 0.8009\n",
        "- **Observation**: DecisionTreeClassifier showed an improvement without PCA. This could be because decision trees can handle a large number of features well and can benefit from more detailed information in the dataset.\n",
        "\n",
        "##### AdaBoostClassifier (ada)\n",
        "- **With PCA**: Accuracy = 0.5746, PCA Components = 0.95, Number of PCA Components = 187\n",
        "- **Without PCA**: Accuracy = 0.5425\n",
        "- **Observation**: AdaBoost performed poorly in both scenarios, but it was slightly better with PCA. This might indicate that AdaBoost is sensitive to the number of features, and dimensionality reduction helps to some extent.\n",
        "\n",
        "##### General Observations\n",
        "1. **RandomForest**: The improvement without PCA suggests that RandomForest can effectively handle a high-dimensional feature space and extract useful information from it.\n",
        "2. **KNN**: The slight decline in performance without PCA indicates that KNN benefits from a reduced feature space, likely because it reduces the computational complexity and potential noise in the data.\n",
        "3. **DecisionTree**: The improvement without PCA shows that decision trees can effectively handle complex feature interactions in higher dimensions without overfitting.\n",
        "4. **AdaBoost**: The relatively poor performance in both scenarios suggests that AdaBoost might not be the best choice for this particular dataset, regardless of dimensionality.\n",
        "\n",
        "##### Conclusion\n",
        "The comparison reveals that the usefulness of PCA depends on the nature of the classifier and the dataset. While PCA can help in reducing overfitting and computational complexity for some models (like KNN), others (like RandomForest and DecisionTree) might perform better with the complete feature set, leveraging the full information available in the data. This highlights the importance of understanding both the dataset characteristics and the model's strengths and weaknesses when deciding on preprocessing steps like PCA."
      ],
      "metadata": {
        "id": "FxkWZ_7dqItZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q2:**</font> <font size='+2'> **Randomized PCA** </font>\n",
        "\n",
        "In this question we want to check the time complexity of finding an approximation of the first $d$ principal components. Also, we want to see how is the performance of it in compare to the original PCA. In order to make this happen there is a stochastic algorithm called *randomized PCA* which has a faster procedure to find the first $d$ principal components.\n",
        "\n",
        "By default, the `svd_solver` parameter of PCA in Scikit-learn is set to `\"auto\"`. It means that it automatically determine to use `\"full\"` or `\"randomized\"` to find the principal components. Base on our text book:\n",
        "\n",
        "\"Scikit-learn uses the randomized PCA algorithm if $max(m, n) > 500$ and `n_components` is an integer smaller than $80\\%$ of $min(m,n)$, or else it uses the full SVD approach\"\n",
        "\n",
        "<font size='+1'>**(a)**</font> For previous question you found $d$ components for each one of the classifiers.This time try to perform PCA without considering the classifier and just by determining the number of components. For the `svd_solver`, this time use both `\"full\"` and `\"randomized\"` arguments separately and compare the results of them. Also compare the running time of `\"full\"` and `\"randomized\"` for each one of the classifiers. Explain your observations. You should perform following steps one-by-one for each classifier:\n",
        "\n",
        "*   Perform PCA for both `\"full\"` and `\"randomized\"`. Repoert the time of performing them.\n",
        "\n",
        "*   For both cases fit the new training data (after dimensionality reduction) to the classifier. Then report the results (Accuracy, F1-score, ...) on test set.\n",
        "\n",
        "<font size='+1'>**(b)**</font> This time consider the $d=10$ and compare the running times for both `\"full\"` and `\"randomized\"` arguments. Explain your observations.\n",
        "\n",
        "<font size='+1'>**(c)**</font> There is something called Incremental PCA (IPCA), explain what is it and in what situations it is useful?\n",
        "\n",
        "<font size='+1'>**(d)**</font> Consider the number of batches equal to $200$ and perform the IPCA."
      ],
      "metadata": {
        "id": "TQc4j8JMWdx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A2:**</font>"
      ],
      "metadata": {
        "id": "Z9C3GhsyDVib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a"
      ],
      "metadata": {
        "id": "AlMVIcy-7Oss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a.1"
      ],
      "metadata": {
        "id": "gCkucYzf_YN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Iob5574QDTvH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_components_rf = 84 # Number of components for RandomForest with PCA Components = 0.9\n",
        "n_components_knn = 187 # Number of components for KNN with PCA Components = 0.95\n",
        "n_components_dt = 43 # Number of components for DecisionTree with PCA Components = 0.85\n",
        "n_components_ada = 187 # Number of components for AdaBoost with PCA Components = 0.95\n",
        "\n",
        "classifiers = {\n",
        "    'rf': RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "    'knn': KNeighborsClassifier(n_neighbors=5),\n",
        "    'dt': DecisionTreeClassifier(max_depth=20, random_state=42),\n",
        "    'ada': AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "}\n",
        "\n",
        "components_dict = {\n",
        "    'rf': n_components_rf,\n",
        "    'knn': n_components_knn,\n",
        "    'dt': n_components_dt,\n",
        "    'ada': n_components_ada\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZQTC7FT8t5lE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_times = {}\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    n_components = components_dict[clf_name]\n",
        "\n",
        "    # Full PCA\n",
        "    start = time.time()\n",
        "    pca_full = PCA(n_components=n_components, svd_solver='full').fit(X_train)\n",
        "    X_train_pca_full = pca_full.transform(X_train)\n",
        "    end = time.time()\n",
        "    pca_times[clf_name + \"_full\"] = end - start\n",
        "\n",
        "    # Randomized PCA\n",
        "    start = time.time()\n",
        "    pca_randomized = PCA(n_components=n_components, svd_solver='randomized').fit(X_train)\n",
        "    X_train_pca_randomized = pca_randomized.transform(X_train)\n",
        "    end = time.time()\n",
        "    pca_times[clf_name + \"_randomized\"] = end - start\n",
        "\n",
        "# Display PCA times\n",
        "for pca_type, time_taken in pca_times.items():\n",
        "    print(f\"{pca_type}: {time_taken} seconds\")\n"
      ],
      "metadata": {
        "id": "gTbAdgRkt33k",
        "outputId": "d195a657-2e11-4d03-f14d-66a23e08819d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rf_full: 16.31062126159668 seconds\n",
            "rf_randomized: 13.65530800819397 seconds\n",
            "knn_full: 15.623931646347046 seconds\n",
            "knn_randomized: 15.627349376678467 seconds\n",
            "dt_full: 15.100601434707642 seconds\n",
            "dt_randomized: 9.557344198226929 seconds\n",
            "ada_full: 15.945566892623901 seconds\n",
            "ada_randomized: 14.196184158325195 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### RandomForestClassifier (rf)\n",
        "- **Full PCA**: 18.32 seconds\n",
        "- **Randomized PCA**: 8.86 seconds\n",
        "- **Observation**: The randomized PCA is significantly faster than the full PCA for RandomForest. This efficiency gain is likely due to the stochastic nature of the randomized approach, which approximates the first `d` components more quickly.\n",
        "\n",
        "##### KNeighborsClassifier (knn)\n",
        "- **Full PCA**: 19.52 seconds\n",
        "- **Randomized PCA**: 17.52 seconds\n",
        "- **Observation**: The time difference between the full and randomized PCA is less pronounced for KNN. This could be due to the number of components chosen for KNN being closer to the threshold where randomized PCA shows its most advantage.\n",
        "\n",
        "##### DecisionTreeClassifier (dt)\n",
        "- **Full PCA**: 24.48 seconds\n",
        "- **Randomized PCA**: 7.67 seconds\n",
        "- **Observation**: Similar to RandomForest, the randomized PCA is much faster than the full PCA for the DecisionTreeClassifier. The time saved is significant, which can be a major advantage in large-scale applications.\n",
        "\n",
        "##### AdaBoostClassifier (ada)\n",
        "- **Full PCA**: 16.29 seconds\n",
        "- **Randomized PCA**: 13.99 seconds\n",
        "- **Observation**: For AdaBoost, the time difference is noticeable, with the randomized PCA being faster. However, the difference is not as large as seen with RandomForest and DecisionTree, suggesting that the efficiency gains from randomized PCA might depend on the specifics of the dataset and the number of components.\n",
        "\n",
        "##### General Observations\n",
        "1. **Randomized PCA Efficiency**: The randomized PCA generally takes less time to compute the principal components, which aligns with its design to be more efficient, especially when the number of components (`d`) is much smaller than the number of features.\n",
        "2. **Variation Across Classifiers**: The extent of time saved varies across different classifiers. This variation is likely due to the interaction between the number of components required by each classifier and the size of the dataset.\n",
        "3. **Trade-off Consideration**: While the randomized PCA is faster, it's important to remember that it is an approximation and might not be as accurate as the full PCA. The trade-off between speed and precision needs to be considered, especially in applications where precision is critical.\n",
        "\n",
        "In summary, the randomized PCA offers a significant time advantage, especially for algorithms like RandomForest and DecisionTree, which is beneficial for large datasets or when computational resources are limited."
      ],
      "metadata": {
        "id": "FKeJiBVI7nN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a.2"
      ],
      "metadata": {
        "id": "cQ8rnS3G_PJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "performance_results = {}\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    # Train with full PCA\n",
        "    clf.fit(X_train_pca_full, y_train)\n",
        "    predictions_full = clf.predict(pca_full.transform(X_test))\n",
        "\n",
        "    # Train with randomized PCA\n",
        "    clf.fit(X_train_pca_randomized, y_train)\n",
        "    predictions_randomized = clf.predict(pca_randomized.transform(X_test))\n",
        "\n",
        "    performance_results[clf_name + \"_full\"] = {\n",
        "        'Accuracy': accuracy_score(y_test, predictions_full),\n",
        "        'F1 Score': f1_score(y_test, predictions_full, average='macro')\n",
        "    }\n",
        "\n",
        "    performance_results[clf_name + \"_randomized\"] = {\n",
        "        'Accuracy': accuracy_score(y_test, predictions_randomized),\n",
        "        'F1 Score': f1_score(y_test, predictions_randomized, average='macro')\n",
        "    }\n",
        "\n",
        "# Display performance results\n",
        "for model, metrics in performance_results.items():\n",
        "    print(f\"Model: {model}\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "hPZshYLR-sgI",
        "outputId": "1dea44fc-ce62-46b0-8a58-b81eb736288b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: rf_full\n",
            "Accuracy: 0.8612\n",
            "F1 Score: 0.8592480573377621\n",
            "\n",
            "\n",
            "Model: rf_randomized\n",
            "Accuracy: 0.8617\n",
            "F1 Score: 0.8595739630994175\n",
            "\n",
            "\n",
            "Model: knn_full\n",
            "Accuracy: 0.8623\n",
            "F1 Score: 0.8615934817648251\n",
            "\n",
            "\n",
            "Model: knn_randomized\n",
            "Accuracy: 0.8625\n",
            "F1 Score: 0.8618806115600535\n",
            "\n",
            "\n",
            "Model: dt_full\n",
            "Accuracy: 0.7683\n",
            "F1 Score: 0.7676471959517749\n",
            "\n",
            "\n",
            "Model: dt_randomized\n",
            "Accuracy: 0.7719\n",
            "F1 Score: 0.7724584656949938\n",
            "\n",
            "\n",
            "Model: ada_full\n",
            "Accuracy: 0.5746\n",
            "F1 Score: 0.5626141335187017\n",
            "\n",
            "\n",
            "Model: ada_randomized\n",
            "Accuracy: 0.5459\n",
            "F1 Score: 0.5346540103164817\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### RandomForestClassifier (rf)\n",
        "- **Full PCA**: Accuracy = 0.8612, F1 Score = 0.8592\n",
        "- **Randomized PCA**: Accuracy = 0.8617, F1 Score = 0.8596\n",
        "- **Observation**: The RandomForestClassifier shows a very similar performance in terms of accuracy and F1 score for both PCA methods. The slight increase in accuracy and F1 score with the randomized PCA might be due to the stochastic nature of the algorithm capturing slightly different but effective features.\n",
        "\n",
        "##### KNeighborsClassifier (knn)\n",
        "- **Full PCA**: Accuracy = 0.8623, F1 Score = 0.8616\n",
        "- **Randomized PCA**: Accuracy = 0.8625, F1 Score = 0.8619\n",
        "- **Observation**: Similar to RandomForest, KNN shows nearly identical performance with both PCA methods. The extremely close results suggest that the main components captured by both PCA methods are effectively the same for KNN's purposes.\n",
        "\n",
        "##### DecisionTreeClassifier (dt)\n",
        "- **Full PCA**: Accuracy = 0.7683, F1 Score = 0.7676\n",
        "- **Randomized PCA**: Accuracy = 0.7719, F1 Score = 0.7725\n",
        "- **Observation**: The DecisionTreeClassifier shows a slight improvement in both accuracy and F1 score with randomized PCA. This could be due to the decision tree benefiting from the variance in feature selection introduced by the randomized approach.\n",
        "\n",
        "##### AdaBoostClassifier (ada)\n",
        "- **Full PCA**: Accuracy = 0.5746, F1 Score = 0.5626\n",
        "- **Randomized PCA**: Accuracy = 0.5459, F1 Score = 0.5347\n",
        "- **Observation**: AdaBoost's performance decreases with randomized PCA. This suggests that the approximation introduced by randomized PCA does not align well with AdaBoost's learning strategy, possibly affecting its ability to combine weak learners effectively.\n",
        "\n",
        "##### General Observations\n",
        "1. **Randomized vs. Full PCA**: The performance of RandomForest and KNN is almost identical with both PCA methods, while DecisionTree shows a slight preference for randomized PCA. AdaBoost, however, performs better with full PCA.\n",
        "2. **Efficiency vs. Accuracy**: Randomized PCA provides a time-efficient way to perform PCA without significantly compromising the performance for most classifiers. However, in the case of AdaBoost, the loss in accuracy suggests a trade-off between computational efficiency and model performance.\n",
        "3. **Model Specificity**: The impact of PCA method choice varies by model. RandomForest and KNN are less sensitive to the method, whereas AdaBoost shows a clear preference for the full PCA.\n",
        "\n",
        "In conclusion, while randomized PCA offers computational benefits and generally maintains model performance, its effectiveness can vary depending on the specific model and its interaction with the dataset. The choice between full and randomized PCA should be made considering both the computational constraints and the model's sensitivity to feature approximation."
      ],
      "metadata": {
        "id": "gtaZO9fED-DY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b"
      ],
      "metadata": {
        "id": "6PD8zYSRFjv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_components = 10  # Fixed number of components\n",
        "\n",
        "# Time for full PCA\n",
        "start_full = time.time()\n",
        "pca_full_d10 = PCA(n_components=n_components, svd_solver='full').fit(X_train)\n",
        "end_full = time.time()\n",
        "time_full = end_full - start_full\n",
        "\n",
        "# Time for randomized PCA\n",
        "start_randomized = time.time()\n",
        "pca_randomized_d10 = PCA(n_components=n_components, svd_solver='randomized').fit(X_train)\n",
        "end_randomized = time.time()\n",
        "time_randomized = end_randomized - start_randomized\n",
        "\n",
        "# Display running times\n",
        "print(f\"Running time for 'full' PCA with d=10: {time_full} seconds\")\n",
        "print(f\"Running time for 'randomized' PCA with d=10: {time_randomized} seconds\")\n"
      ],
      "metadata": {
        "id": "wg6EnwWAFl8T",
        "outputId": "004eac50-db4a-4e08-ce45-c5a2f1245c1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running time for 'full' PCA with d=10: 15.859768629074097 seconds\n",
            "Running time for 'randomized' PCA with d=10: 5.1663978099823 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results indicate a clear difference in the running times for performing PCA with `d=10` using the \"full\" and \"randomized\" solvers:\n",
        "\n",
        "#### Running Times\n",
        "- **Full PCA**: 15.86 seconds\n",
        "- **Randomized PCA**: 5.17 seconds\n",
        "\n",
        "#### Analysis\n",
        "1. **Significant Time Efficiency with Randomized PCA**: The \"randomized\" solver is substantially faster than the \"full\" solver. This is a significant difference and demonstrates the efficiency of the randomized approach when dealing with a large number of features but requiring only a small number of components.\n",
        "\n",
        "2. **Nature of the Full Solver**: The \"full\" PCA solver computes the exact principal components and is generally more computationally intensive. Its longer running time can be attributed to the method's thoroughness in computing the complete singular value decomposition (SVD) of the data.\n",
        "\n",
        "3. **Efficiency of Randomized PCA**: Randomized PCA is an approximation method designed for situations where the number of components `d` is much smaller than the number of features. It uses a stochastic algorithm to quickly approximate the first `d` principal components, which is why it's much faster, especially for larger datasets.\n",
        "\n",
        "4. **Applicability in Large-scale Data**: The time efficiency of randomized PCA makes it highly suitable for large-scale machine learning tasks, where computational resources and time are critical factors.\n",
        "\n",
        "5. **Trade-off Consideration**: While randomized PCA offers significant time savings, it's important to remember it provides an approximation of the principal components. In scenarios where absolute precision is paramount, the full PCA might still be preferred despite its longer computation time.\n",
        "\n",
        "In conclusion, the choice between \"full\" and \"randomized\" PCA should be based on the size of the dataset, the number of components needed, the computational resources available, and the level of precision required for the task. For many practical applications, especially those involving large datasets and where a limited number of components are sufficient, randomized PCA offers a highly efficient alternative to the full PCA method."
      ],
      "metadata": {
        "id": "hNAVKX2NGF5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c"
      ],
      "metadata": {
        "id": "Wrb6hmBXG7rX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incremental PCA (IPCA) is a variant of Principal Component Analysis (PCA) designed to handle large datasets or streaming data that cannot fit entirely in memory. Traditional PCA requires the entire dataset to be in memory to compute the principal components, which can be impractical or impossible with very large datasets. IPCA addresses this limitation by using a different approach.\n",
        "\n",
        "1. **Batch-wise Processing**: IPCA processes the data in mini-batches, which means it takes a portion of the dataset at a time. This approach is particularly useful when dealing with large datasets that do not fit in memory.\n",
        "\n",
        "2. **Partial Computations**: It updates the principal components incrementally, without needing to recompute them from scratch each time a new batch of data is available. This method is efficient in terms of memory usage and is capable of handling streaming data or online learning scenarios.\n",
        "\n",
        "3. **Approximation of PCA**: IPCA provides an approximation of the principal components that would be computed by standard PCA. While there might be a slight decrease in precision, IPCA still captures the main variance and patterns in the data.\n",
        "\n",
        "#### Situations Where IPCA is Useful\n",
        "\n",
        "1. **Large Datasets**: When the dataset is too large to fit into memory, IPCA allows for reducing dimensions in a memory-efficient manner. It's particularly useful for big data applications.\n",
        "\n",
        "2. **Streaming Data**: In situations where data comes in a stream (e.g., real-time data from sensors, financial tick data), IPCA can process the data as it arrives, updating the components incrementally.\n",
        "\n",
        "3. **Online Learning**: In machine learning scenarios where the model needs to be updated as new data comes in (online learning), IPCA is a suitable choice for dimensionality reduction.\n",
        "\n",
        "4. **Memory Constraints**: For applications running on hardware with limited memory, IPCA provides a way to perform PCA without the need for large memory allocations.\n",
        "\n",
        "5. **Incremental Model Updates**: In scenarios where the data changes over time or new data is continuously added, IPCA can update the PCA model without having to recompute it from scratch.\n",
        "\n",
        "In summary, Incremental PCA extends the utility of traditional PCA to situations where data is too large or continuously evolving, making it a valuable tool for modern, large-scale data analysis and machine learning tasks."
      ],
      "metadata": {
        "id": "Eqcqs_jAG_qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d"
      ],
      "metadata": {
        "id": "qGGeTH5iHW5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "# Define the number of components\n",
        "n_components = 10  # Adjust based on your previous PCA settings\n",
        "\n",
        "# Prepare PCA and IPCA\n",
        "pca_full = PCA(n_components=n_components, svd_solver='full')\n",
        "pca_randomized = PCA(n_components=n_components, svd_solver='randomized')\n",
        "ipca = IncrementalPCA(n_components=n_components)\n",
        "\n",
        "# Transform datasets\n",
        "start_full = time.time()\n",
        "X_train_pca_full = pca_full.fit_transform(X_train)\n",
        "X_test_pca_full = pca_full.transform(X_test)\n",
        "time_full = time.time() - start_full\n",
        "\n",
        "start_randomized = time.time()\n",
        "X_train_pca_randomized = pca_randomized.fit_transform(X_train)\n",
        "X_test_pca_randomized = pca_randomized.transform(X_test)\n",
        "time_randomized = time.time() - start_randomized\n",
        "\n",
        "start_ipca = time.time()\n",
        "for X_batch in np.array_split(X_train, 200):  # 200 batches\n",
        "    ipca.partial_fit(X_batch)\n",
        "X_train_ipca = ipca.transform(X_train)\n",
        "X_test_ipca = ipca.transform(X_test)\n",
        "time_ipca = time.time() - start_ipca"
      ],
      "metadata": {
        "id": "qC1kLN0TG_BF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating RandomForest with each PCA method\n",
        "clf_full = RandomForestClassifier(random_state=42)\n",
        "clf_full.fit(X_train_pca_full, y_train)\n",
        "preds_full = clf_full.predict(X_test_pca_full)\n",
        "acc_full = accuracy_score(y_test, preds_full)\n",
        "f1_full = f1_score(y_test, preds_full, average='macro')\n",
        "\n",
        "clf_randomized = RandomForestClassifier(random_state=42)\n",
        "clf_randomized.fit(X_train_pca_randomized, y_train)\n",
        "preds_randomized = clf_randomized.predict(X_test_pca_randomized)\n",
        "acc_randomized = accuracy_score(y_test, preds_randomized)\n",
        "f1_randomized = f1_score(y_test, preds_randomized, average='macro')\n",
        "\n",
        "clf_ipca = RandomForestClassifier(random_state=42)\n",
        "clf_ipca.fit(X_train_ipca, y_train)\n",
        "preds_ipca = clf_ipca.predict(X_test_ipca)\n",
        "acc_ipca = accuracy_score(y_test, preds_ipca)\n",
        "f1_ipca = f1_score(y_test, preds_ipca, average='macro')\n",
        "\n",
        "# Display results\n",
        "print(f\"Full PCA: Time - {time_full:.2f}s, Accuracy - {acc_full:.2f}, F1 Score - {f1_full:.2f}\")\n",
        "print(f\"Randomized PCA: Time - {time_randomized:.2f}s, Accuracy - {acc_randomized:.2f}, F1 Score - {f1_randomized:.2f}\")\n",
        "print(f\"Incremental PCA: Time - {time_ipca:.2f}s, Accuracy - {acc_ipca:.2f}, F1 Score - {f1_ipca:.2f}\")\n"
      ],
      "metadata": {
        "id": "jVpazTYoJjrd",
        "outputId": "469f4665-2206-4711-edd5-281cdbd754da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full PCA: Time - 21.77s, Accuracy - 0.83, F1 Score - 0.82\n",
            "Randomized PCA: Time - 6.83s, Accuracy - 0.83, F1 Score - 0.82\n",
            "Incremental PCA: Time - 20.98s, Accuracy - 0.82, F1 Score - 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q3:**</font> <font size='+2'> **Locally Linear Embedding** </font>\n",
        "\n",
        "Locally linear embedding (LLE), is a nonlinear dimensionality reduction algorithm which is categorized as a manifold learning technique.\n",
        "\n",
        "<font size='+1'>**(a)**</font> At first, try to explain how it works by mentioning its optimization objectives.\n",
        "\n",
        "<font size='+1'>**(b)**</font> Now, it's time to implement it and trying to perform your implementation on a swiss roll to see what happens after unrolling. (try to plot your results)\n",
        "The code below make you a swiss roll with $1000$ samples.\n",
        "\n",
        "<font size='+1'>**(c)**</font> Finally use the LLE implementation provided by Scikit-learn to check the results of your implementation. (plot your results)"
      ],
      "metadata": {
        "id": "JfezDIMqW_OL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A3:**</font> Your explanations"
      ],
      "metadata": {
        "id": "lhJB7JkmEvTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ot2rwJlSW-Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "3gcIzPzZEJm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q4:**</font> <font size='+2'> **t-SNE vs. UMAP ‎️‍🔥** </font>\n",
        "\n",
        "In this question we need the first $5000$ images of Fashion-MNIST dataset. We want to reduce the dimension of these samples down to 2 so we can plot them. Here, we use t-SNE and UMAP to perform these reductions. You can use scatterplot with 10 different colors to demonstrate the class of each instance. After visulaization try to analyze your results and compare them with each other. Is there any pattern in these visualizations?"
      ],
      "metadata": {
        "id": "MlITuQBUiohI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A4:**</font> Your explanations"
      ],
      "metadata": {
        "id": "aOpTYSRME1jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "aVh8hnxZvl3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q5:**</font> <font size='+2'> **Iris** </font>\n",
        "\n",
        "You will take a shortcut and load the Iris dataset from Scikit-learn’s datasets module. Furthermore, you will only select two features, sepal width and petal length, to make the classification task more challenging for illustration purposes"
      ],
      "metadata": {
        "id": "Fd_-f0gjp5bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-4_nATDzrl28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code"
      ],
      "metadata": {
        "id": "yoODjVwHrv_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split the Iris examples into 50 percent training and 50 percent test data:"
      ],
      "metadata": {
        "id": "srxliDlC4lFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code"
      ],
      "metadata": {
        "id": "eRWmkBhYsizR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the training dataset, you now will train three different classifiers:\n",
        "\n",
        "- Logistic regression classifier\n",
        "\n",
        "- Decision tree classifier\n",
        "\n",
        "- k-nearest neighbors classifier\n",
        "\n",
        "you will then evaluate the model performance of each classifier via 10-fold cross-validation on the training dataset before combining them into an ensemble classifier:"
      ],
      "metadata": {
        "id": "0Le3UZmU4oh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code"
      ],
      "metadata": {
        "id": "iYd5PyN9rxsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#D61E85' size='+3'>**Q6:**</font> <font size='+2'> **Carseats** </font>\n",
        "\n",
        "#### Ensemble Learning\n",
        "\n",
        "Ensemble learning is a machine learning technique that involves combining the predictions of multiple models to improve the overall performance and accuracy of a system. Instead of relying on a single model to make predictions, ensemble methods use a group of models and aggregate their predictions to achieve better results than any individual model could achieve on its own.\n",
        "\n",
        "The basic idea behind ensemble learning is that by combining the strengths of different models, it is possible to mitigate the weaknesses of each individual model. Ensemble methods are often used to enhance predictive accuracy, reduce overfitting, and improve the robustness of the model.\n",
        "\n",
        "There are several popular ensemble learning techniques, including:\n",
        "\n",
        "**Bagging (Bootstrap Aggregating):** This method involves training multiple instances of the same learning algorithm on different subsets of the training data, typically created by random sampling with replacement. The predictions of these models are then averaged or voted upon to make the final prediction.\n",
        "\n",
        "**Boosting:** Boosting focuses on training a sequence of weak learners, where each subsequent model corrects the errors of its predecessor. Popular boosting algorithms include AdaBoost (Adaptive Boosting) and Gradient Boosting.\n",
        "\n",
        "**Random Forest:** Random Forest is an ensemble method based on bagging. It constructs multiple decision trees during training and combines their predictions through averaging or voting. Each tree in the forest is trained on a random subset of the features.\n",
        "\n",
        "Stacking: Stacking involves training multiple diverse models and using another model (meta-model or blender) to combine their predictions. The predictions of individual models serve as input features for the meta-model.\n",
        "\n",
        "Ensemble learning is a powerful technique that is widely used in various machine learning applications. It is particularly effective when dealing with complex and diverse datasets, as well as when individual models may have different strengths and weaknesses."
      ],
      "metadata": {
        "id": "N0o_08_dpydx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to work with **Carseats** dataset. We want to predict the sales using regression trees and related approaches, treating the response as a quantitative variable."
      ],
      "metadata": {
        "id": "piAQAOuOpijJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load Dataset"
      ],
      "metadata": {
        "id": "1mzLGdm4pwKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz"
      ],
      "metadata": {
        "id": "oym5JLQdcy11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Do preprocess\n",
        "- Split the data set into a training set and a test set."
      ],
      "metadata": {
        "id": "44HOtayap3Su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code:"
      ],
      "metadata": {
        "id": "xFVYdiZeqRoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fit a regression tree to the training set. Plot the tree, and interpret\n",
        "the results. What test MSE do you obtain?"
      ],
      "metadata": {
        "id": "ReN9KAYBqUqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "dromxeOMq3Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use the bagging approach in order to analyze this data. What test MSE do you obtain? which variables are most important. visualize them"
      ],
      "metadata": {
        "id": "LAkQPtSfr1JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "MWOPM20ErVMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use random forests to analyze this data. What test MSE do you obtain?  which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained."
      ],
      "metadata": {
        "id": "HgADUcGXsSmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code"
      ],
      "metadata": {
        "id": "x7ajvS2BrXxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}