{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hello everyone,\n",
        "\n",
        "Welcome to the Machine Learning (ML) course for the Fall 2023 semester! We want to extend a warm and enthusiastic greeting to each and every one of you.\n",
        "\n",
        "In the coming months, we will investigate a wide range of topics, including different supervised and unsupervised learning methods, and use them on real-world data to explore the challenges people typically encounter in this field.\n",
        "\n",
        "**A quick word on honor code:** Honor codes try to fostering academic honesty, trust, and commitment to uphold ethical standards. Here we provide a small honor code to discourage any kind of plagiarism, cheating and unauthorized collaboration.\n",
        "\n",
        "1. **Implementation Questions**\n",
        "    - You are <u> strictly prohibited </u> to read code snippets from your classmates.\n",
        "    - You are <u> strictly prohibited </u> from *copying* code from any external source, except for Q&A websites like Stack Overflow. If you do use code from such sources, it is imperative to properly attribute and mention the source in your implementation.\n",
        "\n",
        "2. **Non-implementation questions**\n",
        "    - You should use your own words and not just copying the answers from ChatGPT or any other source on the web.\n",
        "    - Please provide the source for any material you are using to answer the questions.\n",
        "    - You are <u> strictly prohibited </u> to read and copy the answers of your classmates.\n",
        "\n",
        "If we suspect that you have violated any of the above rules, you may lose the whole score of that exercise.\n",
        "\n",
        "<font color='RED' size='+1'>**Important Note:**</font> Remember that <u>interpreting and analyzing</u> your code is just as important as the implementation itself. Therefore, it is crucial to not only write code but also to <u>document and provide detailed analysis</u> of the results and insights derived from the code. This analysis should explain the significance of the findings, highlight any patterns or trends observed, and offer interpretations that contribute to a deeper understanding of the data or problem at hand. Additionally, ensure that your code is <u>well-commented</u> to enhance its readability and facilitate comprehension.\n",
        "\n",
        "\n",
        "As your teaching assistants, we are here to support your learning journey throughout the Fall 2023 semester. If you have any questions about the course material, need guidance on assignments, or seek additional resources, please don't hesitate to reach out to us.\n",
        "\n",
        "Best regards, </br>\n",
        "Sina Abbasi, Armin Tourajmehr"
      ],
      "metadata": {
        "id": "ts6G_sYkXR1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End Machine Learning Project\n",
        "\n",
        "For your first exercise, we will go through different steps of an ML task and try to solve a problem on a real-world dataset.\n",
        "This exercise covers the $2^{nd}$ chapter of the *Hands-On Machine Learning* book, which is your main resource for this course. You can refer to the book for more details about each of these steps."
      ],
      "metadata": {
        "id": "GTsNHpO6Qtvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Look at the Big Picture\n",
        "\n"
      ],
      "metadata": {
        "id": "2F7-BcG8RsuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frame the Problem\n",
        "\n",
        "We use ML to solve a problem. This could be detecting whether an email is spam or not, or estimating the price of a house based on its features. As we can see, different problems have different goals, and the goal is not probably just building a model. In an ML task, this goal is called objective, and the objective is always an essential part of any ML task because it determines what we are looking for. Based on that, we can choose the most appropriate algorithm and performance measure to tackle the problem. Another valuable thing to know is the current solution, whether as a clue to solve the task at hand or as a reference for performance.\n",
        "\n",
        "With all this information, you are now ready to start designing your system. First, determine what kind of training supervision the model will need: is it a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement learning task?\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q1:**</font> Explain each of these five kind of supervision in one paragraph. Provide an example for each one of them.\n"
      ],
      "metadata": {
        "id": "wC268ikgSFvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A1:**</font>"
      ],
      "metadata": {
        "id": "cGLjyBhzigUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select a Performance Measure\n",
        "\n",
        "In ML, a performance measure, also known as an evaluation metric, is a measure used to assess the performance or effectiveness of a machine learning model. It quantifies how well the model is able to accomplish its intended task. The choice of performance measure depends on the specific problem and the desired objective."
      ],
      "metadata": {
        "id": "PdNotLKDlFiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "It serves as the foundation for model training and the model looking for the patterns in the data provided in order to make predictions on unseen samples. The data we chose for this exercise is Medical Cost Personal Datasets, which you can learn more about it in the next section.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Dy0mkPmnAFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About the Data\n",
        "\n",
        "The meaning of each column in the dataset is as follows:\n",
        "\n",
        "* **age:** age of primary beneficiary\n",
        "* **sex:** insurance contractor gender, female, male\n",
        "* **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight $(kg / m ^ 2)$ using the ratio of height to weight, ideally 18.5 to 24.9\n",
        "* **children:** Number of children covered by health insurance / Number of dependents\n",
        "* **smoker:** Smoking\n",
        "* **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
        "* **charges:** Individual medical costs billed by health insurance"
      ],
      "metadata": {
        "id": "i3lmC0eRmXr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#D61E85' size='+2'>**Q2:**</font> The data provided for this exercise can be used for both classification and regression tasks. Explain the difference between classification and regression tasks in just one paragraph by making use of examples. Now, consider we are trying to estimate individual medical costs billed by health insurance using the data provided. What type of supervision can we use for it?"
      ],
      "metadata": {
        "id": "2DXd0vYOl-wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A2:**</font>"
      ],
      "metadata": {
        "id": "SREb7s9HjdOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#D61E85' size='+2'>**Q3:**</font> Considering the fact that we want to estimate medical costs billed by health insurance for each person, we know itâ€™s a regression task. The performance measures we can use are MAE, MSE, MAPE, and R2-score. Write down the equation and explain the intuition behind each of them."
      ],
      "metadata": {
        "id": "OdfCvIt_oy5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A3:**</font>"
      ],
      "metadata": {
        "id": "a7C23wpVm6Cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the data is not all that matters. Before you can start to build your model and predict, you need to get insights about data, which could be done using simple methods such as `info()`, `head()`, `describe()`, and `value_counts()`. You can also visualize the data for more insights, which we get to that in the next part of our checklist.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q4:**</font> Try to perform these methods on our dataset and get some insight about it. Is there any other methods to use?\n",
        "\n",
        "<font color='#8FCF26' size='+2'>**A4:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "ywJrhJEZAsH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At last, you need to split the data to train, validation, and test splits. <font color='#D61E85' size='+2'>**Q5:**</font> Why we do this and what is the use of each of them?\n",
        "\n",
        "Rememeber from now on, you must only consider training and validation sets in your analysis and you are not allowed to use the test set except for prediction."
      ],
      "metadata": {
        "id": "uqu3nCKqtAlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A5:**</font>"
      ],
      "metadata": {
        "id": "dfl_mJnq3V6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#D61E85' size='+2'>**Q6:**</font> Split the dataset into training, validation, and test sets using the following ratios: 70% for training, 10% for validation, and 20% for testing.\n",
        "<font color='#8FCF26' size='+2'>**A6:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "6CNuy1HG3c7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\tExplore and Visualize the Data to Gain Insights\n",
        "\n",
        "What we did so far was just a quick glance on the data. Now, it's time to get deeper insights about the data.\n"
      ],
      "metadata": {
        "id": "PVLGSv19-tAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Geographical Data\n",
        "\n",
        "Histograms, boxplot, pie charts, doughnut charts, line charts, multi-series line charts, bar charts, column charts, stacked bar charts, area charts, scatter plots, bubble charts, etc, are among the charts and diagrams you can draw for the dataset.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q7:**</font> Draw some charts and diagrams. You don't need to use all of them, just the ones that will help you to understand the dataset better.\n",
        "\n",
        "<font color='#8FCF26' size='+2'>**A7:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "IEYnHQeI_1oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\tLook for Correlations\n",
        "\n",
        "At first, what is correlation? You can use the video provided below to learn more about Pearson Correlation.\n",
        "\n",
        "[Pearson's Correlation, Clearly Explained!!!](https://www.youtube.com/watch?v=xZ_z8KWkhXE)\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q8:**</font> Try to find the correlation between the `charges` column which we want to predict and every other columns using `corr` method. What is the interpretation of these numbers?\n",
        "\n",
        "<font color='#8FCF26' size='+2'>**A8:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "fYLQ7HNJFued"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#D61E85' size='+2'>**Q9:**</font> This time use `scatter_matrix()` function to find correlation between different columns. Choose four columns of your choice. Try to interpret the diagram you draw. The explanations is more important than the diagram.\n",
        "\n",
        "<font color='#8FCF26' size='+2'>**A9:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "u65LyklxK2v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with Attribute Combinations\n",
        "\n",
        "Feature engineering involves creating new features by transforming, combining, or deriving information from existing features in order to improve the performance of a machine learning model. This process aims to enhance the predictive power or capture specific patterns or relationships that may not be apparent in the original features alone.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q10:**</font> Try to perform feature engineering for our dataset. After creating these new features, it is important to analyze their correlations with the previous features in the dataset. By employing feature engineering methods and examining correlations, we can effectively expand the feature space and potentially uncover meaningful patterns and relationships within the dataset.\n",
        "\n",
        "**Hint:** One straightforward method is using the division operator to derive new features.\n",
        "\n",
        "<font color='#8FCF26' size='+2'>**A10:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "ba5ThdRsNb7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the Data for Machine Learning Algorithms\n",
        "\n"
      ],
      "metadata": {
        "id": "f9IP08ecbgqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning\n",
        "\n",
        "Missing values in datasets pose a significant challenge in machine learning. The presence of dirty data, which requires cleaning, can be categorized into various origins, including inconsistent and incomplete data. Inconsistency encompass different forms of the same word, such as \"US,\" \"USA,\" \"United States,\" and so on.\n",
        "Incompleteness, another category of dirty data, refers to missing or insufficient information. For instance, consider a dataset containing information about individuals, including their age, gender, and occupation. If some entries have missing age values, it exemplifies the issue of incompleteness within the dataset.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q11:**</font> Dirty data have other origins, such as duplicate, incorrect, and inaccurate data. Try to explain the latter two with the help of examples.\n"
      ],
      "metadata": {
        "id": "6YAgQjwwD2xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A11:**</font>"
      ],
      "metadata": {
        "id": "ZVmOfQwqpqJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#D61E85' size='+2'>**Q12:**</font> There are two strategies for dealing with incomplete/missing data: *preserving* data points and *ignoring* the missing data. *Ignoring* missing data by dropping rows and columns containing the missing value is destructive. A better solution could be to try to fill these missing values. What are our options for filling these cells in the data?"
      ],
      "metadata": {
        "id": "hJ0R8OT7tuJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A12:**</font>"
      ],
      "metadata": {
        "id": "8Hol-Gjn0bdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An essential step in data cleaning is to check for validity. What is considered invalid data? check [this link](https://en.wikipedia.org/wiki/Data_cleansing#Data_quality) to find out.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q13:**</font> Using the information provided untill now, try to clean the data provided for this exercise. Try to use `fillna()`, `SimpleImputer`, `KNNImputer`, and `IterativeImputer` as *preserving* approaches. What is your preferred strategy? \"most_frequent\", \"mean\", \"median\", ... . Explain your decision."
      ],
      "metadata": {
        "id": "-s7MNrb4DGu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A13:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "250l3lnSDOfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Text and Categorical Attributes\n",
        "\n",
        "Machine learning algorithms typically require numerical inputs. By encoding categorical variables into numerical representations, we can include them in our models and leverage their information for predictions and analysis.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q14:**</font> Using categorical encoding and one-hot encoding try to transform your dataset, so the model can take advantage of non-numerical attributes. The choice between these two techniques depends on the specific characteristics of the dataset. When would you prefer using one-hot encoding over categorical encoding, or vice versa?\n"
      ],
      "metadata": {
        "id": "L-2WMNyHD84q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A14:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "8Ob5fnEZIXiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling and Transformation\n",
        "\n",
        "According to book:\n",
        "\"There are two common ways to get all attributes to have the same scale: min-max scaling (normalization) and standardization.\"\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q15:**</font> Why we need feature scaling at all? Between these two common ways which one is more robust to outlier? What happened if we donâ€™t do this in terms of convergence of the model?"
      ],
      "metadata": {
        "id": "f7I1Dv0bIyCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A15:**</font>"
      ],
      "metadata": {
        "id": "dytyP3X0Qotx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#D61E85' size='+2'>**Q16:**</font> Try to perform feature scaling to our dataset."
      ],
      "metadata": {
        "id": "D9eDhVZ-RLXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A16:**</font>"
      ],
      "metadata": {
        "id": "pdYdkX4fRYGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\tCustom Transformers\n",
        "\n",
        "\n",
        "*Transformers* definition:\n",
        "\n",
        "> Some estimators (such as a `SimpleImputer`) can also transform a dataset; these are called *transformers*. Once again, the API is simple: the transformation is performed by the `transform()` method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for a `SimpleImputer`. All transformers also have a convenience method called `fit_transform()`, which is equivalent to calling `fit()` and then `transform()` (but sometimes `fit_transform()` is optimized and runs much faster).\n",
        "\n",
        "Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom transformations, cleanup operations, or combining specific attributes. For transformations that donâ€™t require any training, you can just write a function that takes a NumPy array as input and outputs the transformed array. Scikit-learn provides us with `FunctionTransformer` class for these scenarios. Logarithm transformer is one of them which does not need any training.  \n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q17:**</font> What is the use case of logarithm transformer? Try to implement it using `FunctionTransformer` and then use it on our dataset."
      ],
      "metadata": {
        "id": "Gjq7hIcIRwu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A17:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "mAsAk-PuRaoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`FunctionTransformer` is very handy, but what if you would like your transformer to be trainable, learning some parameters in the `fit()` method and using them later in the `transform()` method? For this, you need to write a custom class. Scikit-Learn relies on duck typing, so this class does not have to inherit from any particular base class. All it needs is three methods: `fit()` (which must return self), `transform()`, and `fit_transform()`.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q18:**</font> Robust scaling is a technique that is less sensitive to outliers compared to standardization. This approach is useful when dealing with datasets that contain significant outliers. First, explain how it works and write down its formula. Then try to implement it from scratch by writing a custom class. In addition to `fit()` and `transform()` methods it should contains `inverse_transform()`: executing `scaler.inverse_transform(scaler.fit_transform(X))` should return an array very close to `X`. Then add support for feature names: set `feature_names_in_` in the `fit()` method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the `get_feature_names_out()` method: it should have one optional `input_features=None` argument. If passed, the method should check that its length matches `n_features_in_`, and it should match `feature_names_in_` if it is defined; then `input_features` should be returned. If `input_features` is `None`, then the method should either return `feature_names_in_` if it is defined or `np.array([\"x0\", \"x1\", ...])` with length `n_features_in_` otherwise."
      ],
      "metadata": {
        "id": "hcZyFHs-RaoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A18:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "v14K1yEFgNTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformation Pipelines\n",
        "\n",
        "As you can see, there are many data transformation steps that need to be executed in the right order. Scikit-learn provides the pipeline class to help with such sequence of transformations.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q19:**</font> Write a pipeline for all the transformations you used so far and then call it. Compare your results with previous setup in which you perform the transformations one by one."
      ],
      "metadata": {
        "id": "OxOZrW1HgRtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A19:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "8FwVYbvLugg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select and Train a Model"
      ],
      "metadata": {
        "id": "pc52a0pdue4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate on the Training Set\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q20:**</font> After all itâ€™s time to work with the model. Based on the answers you gave in the beginning of this exercise, you can choose which models you can use for the task. Try to use linear regression, decision tree regressor, random forest regressor, and support vector machine resgressor as your models and train these models on training split which you already created. Remember, using the first six attributes, we want to predict the `charges` column. Using the performance measures we talked about earlier try to compare your results. Donâ€™t bother yourself with underlying dynamics of these models, we get to that in details in future."
      ],
      "metadata": {
        "id": "ViAv7HJ7y6Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A20:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "JUwVY0hZzJff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Better Evaluation Using Cross-Validation\n",
        "\n",
        "As you probably know, you donâ€™t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part of it for model validation. If you have followed the instructions so far, you must have three sets named training, validation, and test. One way to evaluate the models is using the validation set. It helps you decide which model to use. But there is even a better way!\n",
        "In [this](https://www.youtube.com/watch?v=fSytzGwwBVw) short video you can learn about *k-fold cross validation* idea.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q21:**</font> Try to decide which model is the best using the *k-fold cross validation*.\n",
        "\n"
      ],
      "metadata": {
        "id": "J8CEDZIo02IP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A21:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "3BxIdY5y-bcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "Letâ€™s assume that you now have a shortlist of promising models. You now need to hyperparameter-tune them. The two main approaches to do this is grid search and randomized search.\n",
        "\n",
        "<font color='#D61E85' size='+2'>**Q22:**</font> Using these two methods try to improve your 2 best models from the previuos part and then decide which model to use. At the end you can check your model on the test set."
      ],
      "metadata": {
        "id": "Gs4btyS1-0nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='#8FCF26' size='+2'>**A22:**</font> Create some cell down below to write and run your code."
      ],
      "metadata": {
        "id": "fYQFtXbzOyxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Model\n",
        "\n",
        "Itâ€™s often a good idea to save every model you experiment with so\n",
        "that you can come back easily to any model you want. To save a model you can use the `joblib` library."
      ],
      "metadata": {
        "id": "KaEbyylORSLK"
      }
    }
  ]
}